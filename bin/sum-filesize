#!/usr/bin/env bash
#
# sum-filesize - Calculate the cumulative size (in bytes) of regular files from stdin.
#
# Reads newline-delimited file paths from stdin and reports:
#   Total size   (human-readable if numfmt is available)
#   Total files  (count of regular files processed)
#
# Optimisations:
#   * Streaming: avoids retaining the entire file list in RAM (useful for huge inputs)
#   * Parallel path uses a temporary NUL-delimited list + xargs -P for batch stat calls
#   * Robust detection of xargs -P support (exec test instead of parsing help text)
#   * Minimal forks in sequential mode (single while loop)
#
# Usage examples:
#   fd -e png | ./sum-filesize                 # auto parallel
#   rg -0 -l '\\.(png|jpg)$' | tr '\0' '\n' | ./sum-filesize --jobs 12
#   git ls-files | ./sum-filesize --no-parallel
#
# Options:
#   --jobs N        Number of parallel workers (default: CPU count, max 16)
#   -j N            Alias for --jobs
#   --no-parallel   Force sequential mode (disables xargs -P)
#   --help, -h      Show this help
#
# Notes / Limitations:
#   * Filenames containing newlines are not supported (stdin expects one path per line)
#   * Only regular files (-f) counted; others are skipped silently
#   * Uses stat -c %s (GNU) or stat -f %z (BSD/macOS) depending on availability
#   * Parallel mode stages a temporary NUL list (disk I/O is cheaper than large shell arrays)
#
set -euo pipefail

print_help() {
  sed -n '2,/^set -euo/p' "$0" | sed '$d'
}

jobs_auto() {
  if command -v nproc >/dev/null 2>&1; then nproc;
  elif command -v sysctl >/dev/null 2>&1; then sysctl -n hw.ncpu 2>/dev/null || echo 4;
  else echo 4; fi
}

JOBS=""
PARALLEL=1

while [[ $# -gt 0 ]]; do
  case "$1" in
    --jobs|-j)
      JOBS=${2:-}
      [[ -n "$JOBS" ]] || { echo "Missing value for $1" >&2; exit 1; }
      shift 2
      ;;
    --no-parallel)
      PARALLEL=0; shift ;;
    --help|-h)
      print_help; exit 0 ;;
    *)
      echo "Unknown option: $1" >&2; exit 1 ;;
  esac
done

if [[ -z "${JOBS}" ]]; then
  JOBS=$(jobs_auto)
fi

# Cap jobs to something reasonable
if [[ "$JOBS" -gt 16 ]]; then JOBS=16; fi
if [[ "$JOBS" -lt 1 ]]; then JOBS=1; fi

# Decide stat variant
if stat -c %s "$0" >/dev/null 2>&1; then
  STAT_CMD=(stat -c %s)
elif stat -f %z "$0" >/dev/null 2>&1; then
  STAT_CMD=(stat -f %z)
else
  echo "Unsupported stat implementation" >&2
  exit 1
fi

total=0
file_count=0

# Detect xargs -P support (portable: try executing a no-op). If it fails, we won't use parallel.
HAVE_XARGS_P=0
if xargs -P 1 </dev/null 2>/dev/null; then
  HAVE_XARGS_P=1
fi

if [[ $PARALLEL -eq 1 && $HAVE_XARGS_P -eq 1 ]]; then
  tmp_list=$(mktemp -t sum-filesize.XXXXXX)
  trap 'rm -f "$tmp_list"' EXIT
  # Build NUL-delimited list while counting regular files.
  while IFS= read -r path; do
    [[ -f "$path" ]] || continue
    printf '%s\0' "$path" >> "$tmp_list"
    ((file_count++)) || true
  done
  if [[ $file_count -eq 0 ]]; then
    echo "No regular files found."; exit 0
  fi
  sizes=$(xargs -0 -n1000 -P "$JOBS" "${STAT_CMD[@]}" <"$tmp_list" 2>/dev/null || true)
  if [[ -n "$sizes" ]]; then
    total=$(printf '%s\n' "$sizes" | awk 'NF {s+=$1} END{print s+0}')
  fi
else
  # Sequential streaming fallback (no temp file, no array growth)
  while IFS= read -r path; do
    [[ -f "$path" ]] || continue
    size=$("${STAT_CMD[@]}" "$path" 2>/dev/null || echo 0)
    total=$((total + size))
    ((file_count++)) || true
  done
  if [[ $file_count -eq 0 ]]; then
    echo "No regular files found."; exit 0
  fi
fi

# Print result in human readable format
if command -v numfmt >/dev/null 2>&1; then
  human=$(numfmt --to=iec --suffix=B "$total")
  echo "Total size:  $human"
else
  echo "Total size:  $total bytes"
fi

echo "Total files: $file_count"

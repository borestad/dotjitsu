#!/usr/bin/env bash
#
# sum-filesize - Calculate the total size (in bytes) of regular files read from stdin.
#
# Reads newline-delimited file paths from stdin and reports:
#   Total size   (human-readable if numfmt is available)
#   Total files  (count of regular files processed)
#
# Optimisations:
#   * Streaming: avoids retaining the entire file list in RAM (useful for huge inputs)
#   * Parallel path uses a temporary NUL-delimited list + xargs -P for batch stat calls
#   * Robust detection of xargs -P support (exec test instead of parsing help text)
#   * Minimal forks in sequential mode (single while loop)
#
# Usage examples:
#   fd -e png | ./sum-filesize                 # auto parallel
#   rg -0 -l '\\.(png|jpg)$' | tr '\0' '\n' | ./sum-filesize --jobs 12
#   git ls-files | ./sum-filesize --no-parallel
#
# Options:
#   --jobs N        Number of parallel workers (default: CPU count, max 16)
#   -j N            Alias for --jobs
#   --no-parallel   Force sequential mode (disables xargs -P)
#   --help, -h      Show this help
#
# Notes / Limitations:
#   * Filenames containing newlines are not supported (stdin expects one path per line)
#   * Only regular files (-f) counted; others are skipped silently
#   * Uses stat -c %s (GNU) or stat -f %z (BSD/macOS) depending on availability
#   * Parallel mode stages a temporary NUL list (disk I/O is cheaper than large shell arrays)
#
set -euo pipefail

print_help() {
  sed -n '2,/^set -euo/p' "$0" | sed '$d'
}

jobs_auto() {
  if command -v nproc >/dev/null 2>&1; then nproc;
  elif command -v sysctl >/dev/null 2>&1; then sysctl -n hw.ncpu 2>/dev/null || echo 4;
  else echo 4; fi
}

JOBS=""
PARALLEL=1

while [[ $# -gt 0 ]]; do
  case "$1" in
    --jobs|-j)
      JOBS=${2:-}
      [[ -n "$JOBS" ]] || { echo "Missing value for $1" >&2; exit 1; }
      shift 2
      ;;
    --no-parallel)
      PARALLEL=0; shift ;;
    --help|-h)
      print_help; exit 0 ;;
    *)
      echo "Unknown option: $1" >&2; exit 1 ;;
  esac
done

if [[ -z "${JOBS}" ]]; then
  JOBS=$(jobs_auto)
fi

# Cap jobs to something reasonable
if [[ "$JOBS" -gt 16 ]]; then JOBS=16; fi
if [[ "$JOBS" -lt 1 ]]; then JOBS=1; fi

# Decide stat variant
if stat -c %s "$0" >/dev/null 2>&1; then
  STAT_CMD=(stat -c %s)
elif stat -f %z "$0" >/dev/null 2>&1; then
  STAT_CMD=(stat -f %z)
else
  echo "Unsupported stat implementation" >&2
  exit 1
fi

total=0
file_count=0

# Detect xargs -P support (portable: try executing a no-op). If it fails, we won't use parallel.
# Explanation:
# - Some platforms' xargs implementations (notably BSD/macOS) don't support -P or behave differently.
# - Instead of parsing `xargs --help` (which is brittle/localized), we attempt to run a benign command
#   using `xargs -P 1` with no input. If the command succeeds (exit status 0), we assume -P is supported.
# - Using a no-op invocation avoids spawning long-lived processes or altering state; it's only a capability test.
HAVE_XARGS_P=0
if xargs -P 1 </dev/null 2>/dev/null; then
  HAVE_XARGS_P=1
fi

if [[ $PARALLEL -eq 1 && $HAVE_XARGS_P -eq 1 ]]; then
  # Parallel path: stage paths to a temporary NUL-delimited file and run xargs -P to stat batches.
  # Reasons for this approach:
  #  - Streaming huge lists directly into xargs with -P can still cause excessive memory usage in some shells.
  #  - Creating a temporary NUL list on disk avoids storing all paths in shell arrays while remaining fast
  #    because disk+sequential IO is cheaper than large memory growth for very large inputs.
  #  - NUL-delimiting avoids issues with spaces in filenames.
  tmp_list=$(mktemp -t sum-filesize.XXXXXX)
  trap 'rm -f "$tmp_list"' EXIT

  # Build NUL-delimited list while counting regular files.
  # Note: We check -f here to only count regular files; symlinks, directories etc. are silently skipped.
  # We append with printf '%s\0' to preserve filenames verbatim (including spaces).
  while IFS= read -r path; do
    [[ -f "$path" ]] || continue
    printf '%s\0' "$path" >> "$tmp_list"
    ((file_count++)) || true
  done

  if [[ $file_count -eq 0 ]]; then
    echo "No regular files found."; exit 0
  fi

  # Invoke xargs to run stat on batches of files in parallel.
  # - -0 : expect NUL-delimited input
  # - -n1000 : pass up to 1000 paths per stat invocation to amortize the cost of spawning stat
  # - -P "$JOBS" : run up to $JOBS processes in parallel
  # We redirect stderr to /dev/null because stat might print errors for transient files; we tolerate them.
  # The final `|| true` prevents the pipeline from failing the script if xargs/stat exit non-zero for some items.
  sizes=$(xargs -0 -n1000 -P "$JOBS" "${STAT_CMD[@]}" <"$tmp_list" 2>/dev/null || true)

  # sizes will be a newline-delimited list of byte counts (one per stat invocation output).
  # Use awk to sum them robustly; NF guard skips any blank lines, and `s+0` ensures a numeric zero is printed
  # when there are no numbers (avoids printing an empty string).
  if [[ -n "$sizes" ]]; then
    total=$(printf '%s\n' "$sizes" | awk 'NF {s+=$1} END{print s+0}')
  fi
else
  # Sequential streaming fallback (no temp file, no array growth)
  # This path is deliberately simple and uses minimal subshells/forks:
  # - We read each path line-by-line and immediately stat it, keeping only two counters in memory.
  # - This reduces peak memory usage and avoids the need for temporary files or parallel orchestration.
  while IFS= read -r path; do
    [[ -f "$path" ]] || continue
    # Call stat for each file; if stat fails for a specific file (e.g. it was removed), we treat its size as 0.
    size=$("${STAT_CMD[@]}" "$path" 2>/dev/null || echo 0)
    total=$((total + size))
    ((file_count++)) || true
  done
  if [[ $file_count -eq 0 ]]; then
    echo "No regular files found."; exit 0
  fi
fi

# Print result in human readable format
if command -v numfmt >/dev/null 2>&1; then
  human=$(numfmt --to=iec --suffix=B "$total")
  echo "Total size:  $human"
else
  echo "Total size:  $total bytes"
fi

echo "Total files: $file_count"
